""" Batch normalization (also called "batch scale") is a technique used to normalize the activations of the layers in a deep learning model, 
    specifically the mean and variance of the activations over a mini-batch of training examples.

    (ref) https://youtu.be/4gal2zIjm3M
    (ref) https://gaussian37.github.io/dl-concept-batchnorm/
"""

